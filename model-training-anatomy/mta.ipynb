{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d65cb5-39c6-44ff-9614-8ab54c483aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from pynvml import *\n",
    "from transformers import TrainingArguments, Trainer, logging, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75608bf1-7b74-4d94-84ce-83b9b870fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa319db2-3a45-48ed-a290-8ccc63f55549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 223 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891bd7f-a3af-474b-a523-7d8dc4ef7044",
   "metadata": {},
   "source": [
    "That looks good: the GPU memory is not occupied as we would expect before we load any models. If that’s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU the kernels are also loaded,which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fa3d22-e534-4400-b853-3d37cf6aecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 322 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones((1, 1)).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c9e82-cb0c-4d93-a1ba-3dcb2ac773d1",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "First, we load the bert-large-uncased model. We load the model weights directly to the GPU so that we can check how much space just the weights use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea3fabc-a1e3-4627-a54e-a8c9d2d016a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at Felladrin/TinyMistral-248M-SFT-v4 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1240 MB.\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\"\n",
    "MODEL_ID = \"Felladrin/TinyMistral-248M-SFT-v4\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL_ID, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_flash_attention_2=True).to(\"cuda\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41cc71",
   "metadata": {},
   "source": [
    "We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17b319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9bd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88f16",
   "metadata": {},
   "source": [
    "## Memory utilization at vanilla training\n",
    "Let’s use the Trainer and train the model without using any GPU performance optimization techniques and a batch size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e56ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer, logging\n",
    "# logging.set_verbosity_error()\n",
    "# training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "# trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "# result = trainer.train()\n",
    "# print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "445f7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c40a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 80.8524, 'train_samples_per_second': 6.333, 'train_steps_per_second': 1.583, 'train_loss': 0.006253509316593409, 'epoch': 1.0}\n",
      "Time: 80.85\n",
      "Samples/second: 6.33\n",
      "GPU memory occupied: 4986 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57111880",
   "metadata": {},
   "source": [
    "Gradient checkpointing offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For an in-depth explanation of gradient checkpointing, refer to this great article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c883dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 110.6139, 'train_samples_per_second': 4.629, 'train_steps_per_second': 1.157, 'train_loss': 2.607703031287656e-08, 'epoch': 1.0}\n",
      "Time: 110.61\n",
      "Samples/second: 4.63\n",
      "GPU memory occupied: 4612 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24c10f",
   "metadata": {},
   "source": [
    "### fp16\n",
    "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54b0cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 52.2573, 'train_samples_per_second': 9.798, 'train_steps_per_second': 2.449, 'train_loss': 6.984919309616089e-10, 'epoch': 1.0}\n",
      "Time: 52.26\n",
      "Samples/second: 9.80\n",
      "GPU memory occupied: 4990 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78bffb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 64.6782, 'train_samples_per_second': 7.916, 'train_steps_per_second': 1.979, 'train_loss': 2.3283064365386963e-10, 'epoch': 1.0}\n",
      "Time: 64.68\n",
      "Samples/second: 7.92\n",
      "GPU memory occupied: 4600 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5ddb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 65.3438, 'train_samples_per_second': 7.835, 'train_steps_per_second': 1.959, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 65.34\n",
      "Samples/second: 7.83\n",
      "GPU memory occupied: 4600 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768b702",
   "metadata": {},
   "source": [
    "### FlashAttention-2\n",
    "FlashAttention-2 is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:\n",
    "1- additionally parallelizing the attention computation over sequence length\n",
    "2- partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
