{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d65cb5-39c6-44ff-9614-8ab54c483aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from pynvml import *\n",
    "from transformers import TrainingArguments, Trainer, logging, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75608bf1-7b74-4d94-84ce-83b9b870fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa319db2-3a45-48ed-a290-8ccc63f55549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 223 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891bd7f-a3af-474b-a523-7d8dc4ef7044",
   "metadata": {},
   "source": [
    "That looks good: the GPU memory is not occupied as we would expect before we load any models. If that’s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU the kernels are also loaded,which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fa3d22-e534-4400-b853-3d37cf6aecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 322 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones((1, 1)).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c9e82-cb0c-4d93-a1ba-3dcb2ac773d1",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "First, we load the bert-large-uncased model. We load the model weights directly to the GPU so that we can check how much space just the weights use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea3fabc-a1e3-4627-a54e-a8c9d2d016a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185cfb10aa2440f882e778e3472bc5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/590 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2effce10dcc044f09d1e2b4d083ea134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/992M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at Felladrin/TinyMistral-248M-SFT-v4 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1150 MB.\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\"\n",
    "MODEL_ID = \"Felladrin/TinyMistral-248M-SFT-v4\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL_ID, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_flash_attention_2=True).to(\"cuda\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41cc71",
   "metadata": {},
   "source": [
    "We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17b319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9bd267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8c919abc946b39e7714924874294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a5103444cf45babafa54c74186b177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8c0a789f7848f291292c58245e2c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f526a70fdd154a6e8870ac12e3446580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/562 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88f16",
   "metadata": {},
   "source": [
    "## Memory utilization at vanilla training\n",
    "Let’s use the Trainer and train the model without using any GPU performance optimization techniques and a batch size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e56ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer, logging\n",
    "# logging.set_verbosity_error()\n",
    "# training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "# trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "# result = trainer.train()\n",
    "# print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445f7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39c40a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 88.7921, 'train_samples_per_second': 5.766, 'train_steps_per_second': 1.442, 'train_loss': 0.006922087166458368, 'epoch': 1.0}\n",
      "Time: 88.79\n",
      "Samples/second: 5.77\n",
      "GPU memory occupied: 4896 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57111880",
   "metadata": {},
   "source": [
    "Gradient checkpointing offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For an in-depth explanation of gradient checkpointing, refer to this great article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c883dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 121.0431, 'train_samples_per_second': 4.23, 'train_steps_per_second': 1.057, 'train_loss': 8.381902283360887e-09, 'epoch': 1.0}\n",
      "Time: 121.04\n",
      "Samples/second: 4.23\n",
      "GPU memory occupied: 4522 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24c10f",
   "metadata": {},
   "source": [
    "### fp16\n",
    "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b0cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39.0002, 'train_samples_per_second': 13.128, 'train_steps_per_second': 3.282, 'train_loss': 0.011264808475971222, 'epoch': 1.0}\n",
      "Time: 39.00\n",
      "Samples/second: 13.13\n",
      "GPU memory occupied: 7916 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0a659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 41.1301, 'train_samples_per_second': 12.448, 'train_steps_per_second': 3.112, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 41.13\n",
      "Samples/second: 12.45\n",
      "GPU memory occupied: 8186 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=2, gradient_accumulation_steps=2, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78bffb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 63.259, 'train_samples_per_second': 8.094, 'train_steps_per_second': 2.023, 'train_loss': 4.656612873077393e-10, 'epoch': 1.0}\n",
      "Time: 63.26\n",
      "Samples/second: 8.09\n",
      "GPU memory occupied: 4510 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ddb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 60.5463, 'train_samples_per_second': 8.456, 'train_steps_per_second': 2.114, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 60.55\n",
      "Samples/second: 8.46\n",
      "GPU memory occupied: 6254 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0793cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 50.2204, 'train_samples_per_second': 10.195, 'train_steps_per_second': 2.549, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 50.22\n",
      "Samples/second: 10.20\n",
      "GPU memory occupied: 6624 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768b702",
   "metadata": {},
   "source": [
    "### FlashAttention-2\n",
    "FlashAttention-2 is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:\n",
    "1- additionally parallelizing the attention computation over sequence length\n",
    "2- partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them\n",
    "\n",
    "FlashAttention-2 currently supports:\n",
    "- Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.\n",
    "- Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).\n",
    "- All head dimensions up to 256. Head dim > 192 backward requires A100/A800 or H100/H800.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47a04b",
   "metadata": {},
   "source": [
    "### Optimizer choice\n",
    "Trainer integrates a variety of optimizers that can be used out of box: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision, adafactor, or adamw_bnb_8bit. More optimizers can be plugged in via a third-party implementation.\n",
    "\n",
    "1. **Adafactor**: Adafactor doesn’t store rolling averages for each element in weight matrices. Instead, it keeps aggregated information (sums of rolling averages row- and column-wise), significantly reducing its footprint. However, compared to Adam, Adafactor may have slower convergence in certain cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3518a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 51.8633, 'train_samples_per_second': 9.872, 'train_steps_per_second': 2.468, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 51.86\n",
      "Samples/second: 9.87\n",
      "GPU memory occupied: 2684 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731090e",
   "metadata": {},
   "source": [
    "2. **8-bit Adam**: Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "634aecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.6657, 'train_samples_per_second': 10.972, 'train_steps_per_second': 2.743, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 46.67\n",
      "Samples/second: 10.97\n",
      "GPU memory occupied: 4818 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adamw_bnb_8bit\", gradient_checkpointing=True, gradient_checkpointing_kwargs={'use_reentrant':False}, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac5094",
   "metadata": {},
   "source": [
    "We can also use a third-party implementation of the 8-bit optimizer for demonstration purposes to see how that can be integrated.\n",
    "We need to initialize the optimizer. This involves two steps:\n",
    "- First, group the model’s parameters into two groups - one where weight decay should be applied, and the other one where it should not. Usually, biases and layer norm parameters are not weight decayed.\n",
    "- Then do some argument housekeeping to use the same parameters as the previously used AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "071d64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from torch import nn\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, gradient_checkpointing=True, gradient_checkpointing_kwargs={'use_reentrant':False}, fp16=True, **default_args)\n",
    "\n",
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "    \"eps\": training_args.adam_epsilon,\n",
    "}\n",
    "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "    optimizer_grouped_parameters,\n",
    "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "    eps=training_args.adam_epsilon,\n",
    "    lr=training_args.learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a649ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 47.0538, 'train_samples_per_second': 10.881, 'train_steps_per_second': 2.72, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 47.05\n",
      "Samples/second: 10.88\n",
      "GPU memory occupied: 3126 MB.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None), tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc6c85",
   "metadata": {},
   "source": [
    "### Data preloading\n",
    "One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default, everything happens in the main process, and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization. Configure the following arguments to reduce the bottleneck:\n",
    "\n",
    "- DataLoader(pin_memory=True, ...) - ensures the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\n",
    "- DataLoader(num_workers=4, ...) - spawn several workers to preload data faster. During training, watch the GPU utilization stats; if it’s far from 100%, experiment with increasing the number of workers. Of course, the problem could be elsewhere, so many workers won’t necessarily lead to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82307c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 56167\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 6807\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 34333\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 4771\n",
      "    })\n",
      "})\n",
      "<s>### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are different types of grass?</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "print(instruct_tune_dataset)\n",
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "print(instruct_tune_dataset)\n",
    "\n",
    "def create_prompt(sample):\n",
    "  bos_token = \"<s>\"\n",
    "  original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "  system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "  response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "  input = sample[\"response\"]\n",
    "  eos_token = \"</s>\"\n",
    "\n",
    "  full_prompt = \"\"\n",
    "  full_prompt += bos_token\n",
    "  full_prompt += \"### Instruction:\"\n",
    "  full_prompt += \"\\n\" + system_message\n",
    "  full_prompt += \"\\n\\n### Input:\"\n",
    "  full_prompt += \"\\n\" + input\n",
    "  full_prompt += \"\\n\\n### Response:\"\n",
    "  full_prompt += \"\\n\" + response\n",
    "  full_prompt += eos_token\n",
    "\n",
    "  return full_prompt\n",
    "print(create_prompt(instruct_tune_dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a507a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_checkpointing=True, \n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False}, \n",
    "    fp16=True, \n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    **default_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "010ee62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b8b89d549440cbb38fe6efeffa7c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/34333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07eef731c9c41ba942301004b2281a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4771 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34333\n",
      "4771\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'formatting_func'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(eval_dataset))\n\u001b[0;32m----> 7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruct_tune_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstruct_tune_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madam_bnb_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     17\u001b[0m print_summary(result)\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'formatting_func'"
     ]
    }
   ],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "train_dataset=instruct_tune_dataset[\"train\"]\n",
    "eval_dataset=instruct_tune_dataset[\"test\"]\n",
    "# print the length of the train dataset and the test dataset\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=instruct_tune_dataset[\"train\"], \n",
    "    eval_dataset=instruct_tune_dataset[\"test\"],\n",
    "    formatting_func=create_prompt,\n",
    "    max_seq_length=max_seq_length,\n",
    "    optimizers=(adam_bnb_optim, None), \n",
    "    tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
