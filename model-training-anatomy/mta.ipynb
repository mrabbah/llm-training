{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d65cb5-39c6-44ff-9614-8ab54c483aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (/home/mrabbah/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from pynvml import *\n",
    "from transformers import TrainingArguments, Trainer, logging, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75608bf1-7b74-4d94-84ce-83b9b870fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa319db2-3a45-48ed-a290-8ccc63f55549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 223 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891bd7f-a3af-474b-a523-7d8dc4ef7044",
   "metadata": {},
   "source": [
    "That looks good: the GPU memory is not occupied as we would expect before we load any models. If that’s not the case on your machine make sure to stop all processes that are using GPU memory. However, not all free GPU memory can be used by the user. When a model is loaded to the GPU the kernels are also loaded,which can take up 1-2GB of memory. To see how much it is we load a tiny tensor into the GPU which triggers the kernels to be loaded as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1fa3d22-e534-4400-b853-3d37cf6aecdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 322 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones((1, 1)).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c9e82-cb0c-4d93-a1ba-3dcb2ac773d1",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "First, we load the bert-large-uncased model. We load the model weights directly to the GPU so that we can check how much space just the weights use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea3fabc-a1e3-4627-a54e-a8c9d2d016a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at Felladrin/TinyMistral-248M-SFT-v4 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1240 MB.\n"
     ]
    }
   ],
   "source": [
    "# MODEL_ID = \"TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\"\n",
    "MODEL_ID = \"Felladrin/TinyMistral-248M-SFT-v4\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     MODEL_ID, \n",
    "#     torch_dtype=torch.float16,\n",
    "#     use_flash_attention_2=True).to(\"cuda\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(\"cuda\")\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41cc71",
   "metadata": {},
   "source": [
    "We get the same number as before and you can also see that we are using a V100 GPU with 16GB of memory. So now we can start training the model and see how the GPU memory consumption changes. First, we set up a few standard training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17b319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9bd267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88f16",
   "metadata": {},
   "source": [
    "## Memory utilization at vanilla training\n",
    "Let’s use the Trainer and train the model without using any GPU performance optimization techniques and a batch size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e56ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer, logging\n",
    "# logging.set_verbosity_error()\n",
    "# training_args = TrainingArguments(per_device_train_batch_size=4, **default_args)\n",
    "# trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "# result = trainer.train()\n",
    "# print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445f7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39c40a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 80.8524, 'train_samples_per_second': 6.333, 'train_steps_per_second': 1.583, 'train_loss': 0.006253509316593409, 'epoch': 1.0}\n",
      "Time: 80.85\n",
      "Samples/second: 6.33\n",
      "GPU memory occupied: 4986 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57111880",
   "metadata": {},
   "source": [
    "Gradient checkpointing offers a compromise between these two approaches and saves strategically selected activations throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For an in-depth explanation of gradient checkpointing, refer to this great article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c883dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 110.6139, 'train_samples_per_second': 4.629, 'train_steps_per_second': 1.157, 'train_loss': 2.607703031287656e-08, 'epoch': 1.0}\n",
      "Time: 110.61\n",
      "Samples/second: 4.63\n",
      "GPU memory occupied: 4612 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24c10f",
   "metadata": {},
   "source": [
    "### fp16\n",
    "The main advantage of mixed precision training comes from saving the activations in half precision (fp16). Although the gradients are also computed in half precision they are converted back to full precision for the optimization step so no memory is saved here. While mixed precision training results in faster computations, it can also lead to more GPU memory being utilized, especially for small batch sizes. This is because the model is now present on the GPU in both 16-bit and 32-bit precision (1.5x the original model on the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b0cb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39.0002, 'train_samples_per_second': 13.128, 'train_steps_per_second': 3.282, 'train_loss': 0.011264808475971222, 'epoch': 1.0}\n",
      "Time: 39.00\n",
      "Samples/second: 13.13\n",
      "GPU memory occupied: 7916 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0a659e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 41.1301, 'train_samples_per_second': 12.448, 'train_steps_per_second': 3.112, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 41.13\n",
      "Samples/second: 12.45\n",
      "GPU memory occupied: 8186 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=2, gradient_accumulation_steps=2, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78bffb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.9263, 'train_samples_per_second': 10.911, 'train_steps_per_second': 2.728, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 46.93\n",
      "Samples/second: 10.91\n",
      "GPU memory occupied: 6938 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ddb327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 60.5463, 'train_samples_per_second': 8.456, 'train_steps_per_second': 2.114, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 60.55\n",
      "Samples/second: 8.46\n",
      "GPU memory occupied: 6254 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0793cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 50.2204, 'train_samples_per_second': 10.195, 'train_steps_per_second': 2.549, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 50.22\n",
      "Samples/second: 10.20\n",
      "GPU memory occupied: 6624 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768b702",
   "metadata": {},
   "source": [
    "### FlashAttention-2\n",
    "FlashAttention-2 is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:\n",
    "1- additionally parallelizing the attention computation over sequence length\n",
    "2- partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them\n",
    "\n",
    "FlashAttention-2 currently supports:\n",
    "- Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100). Support for Turing GPUs (T4, RTX 2080) is coming soon, please use FlashAttention 1.x for Turing GPUs for now.\n",
    "- Datatype fp16 and bf16 (bf16 requires Ampere, Ada, or Hopper GPUs).\n",
    "- All head dimensions up to 256. Head dim > 192 backward requires A100/A800 or H100/H800.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47a04b",
   "metadata": {},
   "source": [
    "### Optimizer choice\n",
    "Trainer integrates a variety of optimizers that can be used out of box: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision, adafactor, or adamw_bnb_8bit. More optimizers can be plugged in via a third-party implementation.\n",
    "\n",
    "1. **Adafactor**: Adafactor doesn’t store rolling averages for each element in weight matrices. Instead, it keeps aggregated information (sums of rolling averages row- and column-wise), significantly reducing its footprint. However, compared to Adam, Adafactor may have slower convergence in certain cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3518a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 51.2032, 'train_samples_per_second': 9.999, 'train_steps_per_second': 2.5, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 51.20\n",
      "Samples/second: 10.00\n",
      "GPU memory occupied: 4374 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adafactor\", gradient_checkpointing=True, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731090e",
   "metadata": {},
   "source": [
    "2. **8-bit Adam**: Instead of aggregating optimizer states like Adafactor, 8-bit Adam keeps the full state and quantizes it. Quantization means that it stores the state with lower precision and dequantizes it only for the optimization. This is similar to the idea behind mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "634aecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.6657, 'train_samples_per_second': 10.972, 'train_steps_per_second': 2.743, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 46.67\n",
      "Samples/second: 10.97\n",
      "GPU memory occupied: 4818 MB.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(per_device_train_batch_size=4, optim=\"adamw_bnb_8bit\", gradient_checkpointing=True, gradient_checkpointing_kwargs={'use_reentrant':False}, fp16=True, **default_args)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cac5094",
   "metadata": {},
   "source": [
    "We can also use a third-party implementation of the 8-bit optimizer for demonstration purposes to see how that can be integrated.\n",
    "We need to initialize the optimizer. This involves two steps:\n",
    "- First, group the model’s parameters into two groups - one where weight decay should be applied, and the other one where it should not. Usually, biases and layer norm parameters are not weight decayed.\n",
    "- Then do some argument housekeeping to use the same parameters as the previously used AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "071d64b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from torch import nn\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=4, gradient_checkpointing=True, gradient_checkpointing_kwargs={'use_reentrant':False}, fp16=True, **default_args)\n",
    "\n",
    "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "        \"weight_decay\": training_args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "    \"eps\": training_args.adam_epsilon,\n",
    "}\n",
    "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "    optimizer_grouped_parameters,\n",
    "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "    eps=training_args.adam_epsilon,\n",
    "    lr=training_args.learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a649ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 45.8609, 'train_samples_per_second': 11.164, 'train_steps_per_second': 2.791, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 45.86\n",
      "Samples/second: 11.16\n",
      "GPU memory occupied: 4818 MB.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None), tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc6c85",
   "metadata": {},
   "source": [
    "### Data preloading\n",
    "One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it can handle. By default, everything happens in the main process, and it might not be able to read the data from disk fast enough, and thus create a bottleneck, leading to GPU under-utilization. Configure the following arguments to reduce the bottleneck:\n",
    "\n",
    "- DataLoader(pin_memory=True, ...) - ensures the data gets preloaded into the pinned memory on CPU and typically leads to much faster transfers from CPU to GPU memory.\n",
    "- DataLoader(num_workers=4, ...) - spawn several workers to preload data faster. During training, watch the GPU utilization stats; if it’s far from 100%, experiment with increasing the number of workers. Of course, the problem could be elsewhere, so many workers won’t necessarily lead to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82307c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 56167\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 6807\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 34333\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 4771\n",
      "    })\n",
      "})\n",
      "<s>### Instruction:\n",
      "Use the provided input to create an instruction that could have been used to generate the response with an LLM.\n",
      "\n",
      "### Input:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n",
      "\n",
      "### Response:\n",
      "What are different types of grass?</s>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")\n",
    "print(instruct_tune_dataset)\n",
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")\n",
    "print(instruct_tune_dataset)\n",
    "\n",
    "def create_prompt(sample):\n",
    "  bos_token = \"<s>\"\n",
    "  original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "  system_message = \"Use the provided input to create an instruction that could have been used to generate the response with an LLM.\"\n",
    "  response = sample[\"prompt\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "  input = sample[\"response\"]\n",
    "  eos_token = \"</s>\"\n",
    "\n",
    "  full_prompt = \"\"\n",
    "  full_prompt += bos_token\n",
    "  full_prompt += \"### Instruction:\"\n",
    "  full_prompt += \"\\n\" + system_message\n",
    "  full_prompt += \"\\n\\n### Input:\"\n",
    "  full_prompt += \"\\n\" + input\n",
    "  full_prompt += \"\\n\\n### Response:\"\n",
    "  full_prompt += \"\\n\" + response\n",
    "  full_prompt += eos_token\n",
    "\n",
    "  return full_prompt\n",
    "print(create_prompt(instruct_tune_dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a507a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4, \n",
    "    gradient_checkpointing=True, \n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False}, \n",
    "    fp16=True, \n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    **default_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "010ee62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py:100: RuntimeWarning: divide by zero encountered in remainder\n",
      "  return table.fast_gather(key % table.num_rows)\n",
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py:100: RuntimeWarning: divide by zero encountered in remainder\n",
      "  return table.fast_gather(key % table.num_rows)\n",
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py:100: RuntimeWarning: divide by zero encountered in remainder\n",
      "  return table.fast_gather(key % table.num_rows)\n",
      "/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py:100: RuntimeWarning: divide by zero encountered in remainder\n",
      "  return table.fast_gather(key % table.num_rows)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2799, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2795, in __getitem__\n    return self._getitem(key)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2779, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 588, in query_table\n    pa_subtable = _query_table_with_indices_mapping(table, key, indices=indices)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 75, in _query_table_with_indices_mapping\n    return _query_table(table, [indices.fast_slice(i, 1).column(0)[0].as_py() for i in key])\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 100, in _query_table\n    return table.fast_gather(key % table.num_rows)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/table.py\", line 134, in fast_gather\n    [\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/table.py\", line 135, in <listcomp>\n    self._batches[batch_idx].slice(i - self._offsets[batch_idx], 1)\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39m(adam_bnb_optim, \u001b[38;5;28;01mNone\u001b[39;00m), \n\u001b[1;32m      9\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m---> 10\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m print_summary(result)\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/transformers/trainer.py:1838\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1838\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1839\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/accelerate/data_loader.py:448\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 448\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2799, in __getitems__\n    batch = self.__getitem__(keys)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2795, in __getitem__\n    return self._getitem(key)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2779, in _getitem\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 588, in query_table\n    pa_subtable = _query_table_with_indices_mapping(table, key, indices=indices)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 75, in _query_table_with_indices_mapping\n    return _query_table(table, [indices.fast_slice(i, 1).column(0)[0].as_py() for i in key])\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 100, in _query_table\n    return table.fast_gather(key % table.num_rows)\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/table.py\", line 134, in fast_gather\n    [\n  File \"/home/mrabbah/Documents/projects/github/llm-training/.env/lib/python3.10/site-packages/datasets/table.py\", line 135, in <listcomp>\n    self._batches[batch_idx].slice(i - self._offsets[batch_idx], 1)\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=instruct_tune_dataset[\"train\"], \n",
    "    eval_dataset=instruct_tune_dataset[\"test\"],\n",
    "    # formatting_func=create_prompt,\n",
    "    # max_seq_length=max_seq_length,\n",
    "    optimizers=(adam_bnb_optim, None), \n",
    "    tokenizer=tokenizer)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
